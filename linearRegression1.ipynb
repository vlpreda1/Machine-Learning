{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC ML LabExercise - Univariate Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you are going to implement univariate linear regression. You will implement a gradient descent procedure to iteratively search for the solution. \n",
    "$$\n",
    "\\newcommand{\\ls}[1]{{}^{(#1)}}\n",
    "\\renewcommand{\\v}[1]{\\boldsymbol{#1}}\n",
    "\\renewcommand{\\T}{{}^T}\n",
    "\\newcommand{\\matvec}[1]{\\begin{pmatrix}#1\\end{pmatrix}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data $(x\\ls 1,y\\ls 1),\\ldots,(x\\ls m, y\\ls m)$ where the $x$ values are the independent variables, these values are error free. The dependent values $y$ do contain errors.\n",
    "\n",
    "Linear regression fits a model function (*hypothesis*) $h_{\\v\\theta}(x)$ such that the sum of squared errors is minimized:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\v\\theta}(x\\ls i) - y\\ls i)^2\n",
    "$$\n",
    "Linear regression is called *linear* regression because we assume the hypothesis function $h_{\\v\\theta}$ is linear in its parameters $\\v\\theta$:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\theta_0 \\phi_0(x) + \\cdots + \\theta_n \\phi_n(x)\n",
    "$$\n",
    "where $\\phi_0,\\ldots,\\phi_n$ are arbitrary functions in $x$. In case we write:\n",
    "$$\n",
    "\\v x = \\matvec{\\phi_0(x)\\\\\\vdots\\\\\\phi_n(x)}\n",
    "$$\n",
    "the hypothesis function becomes:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\v\\theta\\T \\v x\n",
    "$$\n",
    "and the cost function is:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i)^2\n",
    "$$\n",
    "The gradient is given by:\n",
    "$$\n",
    "\\frac{\\partial J(\\v\\theta)}{\\partial \\v\\theta} =\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i) \\v x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression in Practice I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a simple example. We will generate data with:\n",
    "$$\n",
    "   y\\ls i = a x\\ls i + b + R\n",
    "$$\n",
    "where $R$ is a random variable, i.e. its value is not exactly\n",
    "known. We assume here that $R$ is normally distributed with mean zero\n",
    "and standard deviation 0.3.\n",
    "\n",
    "We collect all values $x\\ls i$ for $i=1,\\ldots,m$ in an array of shape ``(m,)``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x44b7c9f588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 100;\n",
    "a = 0.5\n",
    "b = 2\n",
    "x = linspace(0,10,m)\n",
    "y = a * x + b + 0.3 * random.randn(m)\n",
    "plot(x, y, 'r+');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that $x\\ls i$ is stored in ``x[i]`` (and equivalently for $y$\n",
    "and ``y``).\n",
    "\n",
    "In this case we want to fit a model of the form $h_{\\v\\theta}(x)=a x + b$\n",
    "to the data. Note that with \n",
    "$$\n",
    "   \\v x = \\matvec{1\\\\x}\n",
    "$$\n",
    "(i.e. with $\\phi_0(x)=1$ and $\\phi_1(x)=x$) we have:\n",
    "$$\n",
    "   h_{\\v\\theta}(x) = \\theta_0 + \\theta_1 x\n",
    "$$\n",
    "where $\\theta_0$ is $a$ and $\\theta_0$ is $b$. A constant function\n",
    "$\\phi_0$ in a linear hypothesis (*linear in its parameters!*) is often\n",
    "called a bias term of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``cost(theta, x, y)`` that calculates the cost. Note that ``x`` is the vector with all $x\\ls i$-values and ``y`` is the vector with all $y\\ls i$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.045 0.0358268254195\n"
     ]
    }
   ],
   "source": [
    "def cost(theta0, theta1, x, y):\n",
    "    result = ((theta0 * x[0]) - y[0])**2 + ((theta1 * x[1]) - y[0])**2\n",
    "    return result / (2 * len(x))\n",
    "\n",
    "print(0.3**2/2, cost(2, 0.5, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your function called with ``cost(b,a,x,y)`` (where ``b``, ``a``,\n",
    "``x`` and ``y``) are defined as in the previous code snippet,\n",
    "should a return a value that is close to $0.3^2/2$ (For extra\n",
    "points: can you prove this?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``theta0, theta1 = gradDescentStep(theta0, theta1, x, y)`` that does the calculations for one gradient descent step. In this function we use the Python possibility to return a tuple of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradDescentStep(learningrate, theta0, theta1, x, y):\n",
    "    theta0 -= learningrate * dot((1/len(x) * ((theta0 * x[0]) - y[0])), x[0])\n",
    "    theta1 -= learningrate * dot((1/len(x) * ((theta1 * x[1]) - y[1])), x[1])\n",
    "    return theta0, theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with values ``theta0 = theta1 = 0``. Calculate the costfor these values. After the gradient descent step, using ``learningrate=0.01``, resulting in new theta values again calculate the cost. If all went well the cost should have decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0367826986946 >= 0.0367826509633\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "theta0 = theta1 = 0\n",
    "costbefore = cost(theta0, theta1, x, y)\n",
    "theta0, theta1 = gradDescentStep(0.01, theta0, theta1, x, y)\n",
    "costafter = cost(theta0, theta1, x, y)\n",
    "print(costbefore, '>=', costafter)\n",
    "print(costbefore >= costafter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Problem: Implement Least Squares with closed form solution\n",
    "\n",
    "For the Least Squares method there is also a closed-form solution.\n",
    "\n",
    "$\\theta_1$ can be found by:\n",
    "$$ \\boldsymbol{\\hat\\theta_1} =( X ^TX)^{-1}X^{T}\\boldsymbol y $$\n",
    "\n",
    "You can leave $\\theta_0$ to be 0. Make a plot with your data as dots and your prediction as a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
